{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient_Descent and Cost_function Part-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we know, *y=mx+b* is the formula for regression line\n",
    "### Now, we have to calculate the value for _m and b_\n",
    "# Theory & Algorithm\n",
    "<img src='gd1.jpeg' height=\"600\" width=\"600\">\n",
    "<img src='gd2.png' height=\"600\" width=\"600\">\n",
    "<img src='gd3.png' height=\"600\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####                                    Target is minimizing this cost function\n",
    "<img src='gd4.png' height=\"600\" width=\"600\">\n",
    "<img src='gd5.png' height=\"600\" width=\"600\">\n",
    "#### Fix step size can skip optimal point. So, we have to decrease step size every time\n",
    "#### We have to use derivative. Since, we have multiple variable, we need partial derivatives\n",
    "<img src='gd6.gif'>\n",
    "Now, partial derivatives of m and b\n",
    "<img src='gd7.png' height=\"600\" width=\"600\">\n",
    "Formula for determining m and b\n",
    "<img src='gd8.png' height=\"600\" width=\"600\">\n",
    "<img src='gd9.png' height=\"600\" width=\"600\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
